{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "from math import log10\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import csv\n",
    "import torchmetrics.image as imq\n",
    "import torchmetrics as tm\n",
    "from custom_losses import *\n",
    "from custom_metrics import *\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from train_utils import *\n",
    "from dbpn import BasicNet as BDBPN\n",
    "from data import get_training_set\n",
    "from dataset import *\n",
    "import pdb\n",
    "import socket\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02fd041",
   "metadata": {},
   "source": [
    "#### Model Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of GPUs\n",
    "gpus=1\n",
    "# Whether to use GPU or not\n",
    "gpu_mode=True\n",
    "# Random seed to use.\n",
    "seed=123\n",
    "# Super resolution upscale factor\n",
    "upscale_factor=2\n",
    "# Whether to use pretrained model state\n",
    "pretrained=False\n",
    "# Location to the pretrained models checkpoints\n",
    "save_folder='models/Pretrained/'\n",
    "#Meta data folder\n",
    "meta_folder='Meta-folder/'\n",
    "# Name of the pretrained SR model to load')\n",
    "pretrained_sr='DBPNccp2x-check_epoch_3.pth'\n",
    "# Type of the model to use ', type=str, default='DBPNLL')\n",
    "model_type=\"DBPN\"\n",
    "# Use the below option only if residual learning is desired\n",
    "residual=False\n",
    "# parser.add_argument('--start_iter', type=int, default=1, help='Starting Epoch')\n",
    "start_iter=1\n",
    "# parser.add_argument('--nEpochs', type=int, default=2000, help='number of epochs to train for')\n",
    "nEpochs=100\n",
    "# 'Frequency of storing model checkpoints (after how many epoch hops)\n",
    "snapshots=5\n",
    "# parser.add_argument('--lr', type=float, default=1e-4, help='Learning Rate. Default=0.01')\n",
    "lrate=1e-4\n",
    "# Descriptive Name of model checkpoint\n",
    "prefix='CCPs2x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus_list = range(gpus)\n",
    "hostname = str(socket.gethostname())\n",
    "cudnn.benchmark = True\n",
    "# print(opt)\n",
    "\n",
    "cuda = gpu_mode\n",
    "if cuda and not torch.cuda.is_available():\n",
    "    raise Exception(\"No GPU found, please run without --cuda\")\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a170b3",
   "metadata": {},
   "source": [
    "## Loading the Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('===> Loading datasets')\n",
    "\n",
    "#Select the root directory\n",
    "root='/BioSR/Training/CCPs/'\n",
    "\n",
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the training dataset\n",
    "\n",
    "train_set = ImagePairTrainDataset(root_dir=root,norm_flag=1)\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=1, batch_size=1, shuffle=True)\n",
    "\n",
    "# Load the Validation dataset\n",
    "\n",
    "val_set = ImagePairValidationDataset(root_dir=root,norm_flag=1)\n",
    "val_data_loader = DataLoader(dataset=val_set, num_workers=1, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45148b30",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('===> Building model ', model_type)\n",
    "\n",
    "if model_type == 'DBPN':\n",
    "    model = BDBPN(num_channels=1, base_filter=64,  feat = 256, num_stages=7, scale_factor=upscale_factor) \n",
    "    \n",
    "model = torch.nn.DataParallel(model, device_ids=gpus_list)\n",
    "#criterion = nn.L1Loss()\n",
    "criterion = FDL()\n",
    "\n",
    "print('---------- Networks architecture -------------')\n",
    "#print_network(model)\n",
    "print('----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model = model.cuda(gpus_list[0])\n",
    "    criterion = criterion.cuda(gpus_list[0])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lrate, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "if pretrained:\n",
    "    model_name = os.path.join(save_folder + pretrained_sr)\n",
    "    if os.path.exists(model_name):\n",
    "        #model= torch.load(model_name, map_location=lambda storage, loc: storage)\n",
    "        chk= torch.load(model_name, map_location=lambda storage, loc: storage)\n",
    "        #model.load_state_dict(torch.load(model_name, map_location=lambda storage, loc: storage))\n",
    "        model.load_state_dict(chk['model_state_dict'])\n",
    "        print('Pre-trained SR model is loaded.')\n",
    "        #optimizer.load_state_dict(torch.load(model_name, map_location=lambda storage, loc: storage))\n",
    "        optimizer.load_state_dict(chk['optimizer_state_dict'])\n",
    "        print('Stored optimizer state is loaded.')\n",
    "\n",
    "def checkpoint(epoch):\n",
    "    model_out_path = save_folder+model_type+prefix+\"_epoch_{}.pth\".format(epoch)\n",
    "    torch.save({'model_state_dict': model.state_dict(),\\\n",
    "    'optimizer_state_dict': optimizer.state_dict()}, model_out_path)\n",
    "    #torch.save( PATH)\n",
    "    print(\"Checkpoint saved to {}\".format(model_out_path))\n",
    "\n",
    "def train(epoch):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for iteration, batch in enumerate(training_data_loader, 1):\n",
    "        # Use the below line only if residual learning is desired\n",
    "        #input, target, bicubic = Variable(batch[0]), Variable(batch[1]), Variable(batch[2])\n",
    "        input, target = Variable(batch[0]), Variable(batch[1])\n",
    "        if cuda:\n",
    "            input = input.cuda(gpus_list[0])\n",
    "            target = target.cuda(gpus_list[0])\n",
    "            # Use the below line only if residual learning is desired\n",
    "            # bicubic = bicubic.cuda(gpus_list[0])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        t0 = time.time()\n",
    "        prediction = model(input)\n",
    "\n",
    "        if residual:\n",
    "            prediction = prediction + bicubic\n",
    "\n",
    "        loss = criterion(prediction, target)\n",
    "        t1 = time.time()\n",
    "        epoch_loss += loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(\"===> Epoch[{}]({}/{}): Loss: {:.4f} || Timer: {:.4f} sec.\".format(epoch, iteration, len(training_data_loader), loss.data, (t1 - t0)))\n",
    "\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss / len(training_data_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28476ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    epoch_train_loss = 0\n",
    "    model.train()\n",
    "    for iteration, batch in enumerate(training_data_loader, 1):\n",
    "        # Use the below line only if residual learning is desired\n",
    "        #input, target, bicubic = Variable(batch[0]), Variable(batch[1]), Variable(batch[2])\n",
    "        input, target = Variable(batch[0]), Variable(batch[1])\n",
    "        if cuda:\n",
    "            input = input.cuda(gpus_list[0])\n",
    "            target = target.cuda(gpus_list[0])\n",
    "            # Use the below line only if residual learning is desired\n",
    "            # bicubic = bicubic.cuda(gpus_list[0])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        t0 = time.time()\n",
    "        prediction = model(input)\n",
    "\n",
    "        if residual:\n",
    "            prediction = prediction + bicubic\n",
    "\n",
    "        loss = criterion(prediction, target)\n",
    "        t1 = time.time()\n",
    "        epoch_train_loss += loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(\"===> Epoch[{}]({}/{}): Loss: {:.4f} || Timer: {:.4f} sec.\".format(epoch, iteration, len(training_data_loader), loss.data, (t1 - t0)))\n",
    "\n",
    "    #####################################################Validation Loss##################################################################\n",
    "    model.eval()\n",
    "    epoch_val_loss=[]\n",
    "    epoch_mse=[]\n",
    "    epoch_psnr=[]\n",
    "    epoch_msssim=[]\n",
    "    epoch_nrmse=[]\n",
    "    with torch.no_grad():\n",
    "        for iteration, batch in enumerate(val_data_loader,1):\n",
    "            input,target=Variable(batch[0]), Variable(batch[1])\n",
    "            if cuda:\n",
    "                input=input.cuda(gpus_list[0])\n",
    "                target=target.cuda(gpus_list[0])\n",
    "            \n",
    "            prediction = model(input)\n",
    "            loss=criterion(prediction,target)\n",
    "            epoch_val_loss.append(loss.data)\n",
    "            epoch_mse.append(tm.MeanSquaredError().cuda(gpus_list[0])(percentile_normalize(prediction),target))\n",
    "            #epoch_psnr.append(imq.PeakSignalNoiseRatio().cuda(gpus_list[0])(prediction,target))\n",
    "            epoch_psnr.append(calcPSNR(percentile_normalize(prediction),percentile_normalize(target)))\n",
    "            epoch_msssim.append(imq.StructuralSimilarityIndexMeasure().cuda(gpus_list[0])(percentile_normalize(prediction),target))\n",
    "            epoch_nrmse.append(NormalizedRootMeanSquaredError(normalization='l2').cuda(gpus_list[0])(prediction,target))\n",
    "            \n",
    "    print(\"===> Epoch {} Complete: Avg. Training Loss: {:.4f}, Avg. Validation Loss: {:.4f}, Avg. MSE: {:.4f}, Avg. PSNR: {:.4f}\\\n",
    "          , Avg. SSIM: {:.4f}, Avg. NRMSE: {:.4f}\".\\\n",
    "          format(epoch, epoch_train_loss / len(training_data_loader),sum(epoch_val_loss)/len(val_data_loader), \\\n",
    "                 sum(epoch_mse)/len(val_data_loader), sum(epoch_psnr)/len(val_data_loader),\\\n",
    "                    sum(epoch_msssim)/len(val_data_loader),sum(epoch_nrmse)/len(val_data_loader)))\n",
    "    return epoch_train_loss / len(training_data_loader),sum(epoch_val_loss)/len(val_data_loader), sum(epoch_mse)/len(val_data_loader),\\\n",
    "                 sum(epoch_psnr)/len(val_data_loader),sum(epoch_msssim)/len(val_data_loader),sum(epoch_nrmse)/len(val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "val_losses=[]\n",
    "mses=[]\n",
    "psnrs=[]\n",
    "msssims=[]\n",
    "nrmses=[]\n",
    "\n",
    "epoch_train_loss=0\n",
    "epoch_val_loss=0\n",
    "epoch_mse=0\n",
    "epoch_psnr=0\n",
    "epoch_msssim=0\n",
    "epoch_nrmse=0\n",
    "for epoch in range(start_iter, nEpochs + 1):\n",
    "    print(\"Starting Epoch {}\".format(epoch))\n",
    "    epoch_train_loss, epoch_val_loss, epoch_mse, epoch_psnr, epoch_msssim, epoch_nrmse=train(epoch)\n",
    "    print(\"Epoch {} Completed\".format(epoch))\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    mses.append(epoch_mse)\n",
    "    psnrs.append(epoch_psnr)\n",
    "    msssims.append(epoch_msssim)\n",
    "    nrmses.append(epoch_nrmse)\n",
    "    # learning rate is decayed by a factor of 10 every half of total epochs\n",
    "    if (epoch+1) % (nEpochs/2) == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] /= 10.0\n",
    "        print('Learning rate decay: lr={}'.format(optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "    if (epoch+1) % (snapshots) == 0:\n",
    "        checkpoint(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint(epoch)\n",
    "with open(meta_folder+model_type+prefix+'losses_metrics.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Epoch_Number', 'Training_Loss', 'Validation_Loss', 'MSE', 'PSNR', 'MSSSIM', 'NRMSE'])\n",
    "    for epoch, (t_loss, v_loss,mse,psnr,msssim,nrmse) in \\\n",
    "        enumerate(zip(train_losses, val_losses,mses, psnrs, msssims, nrmses), 1):\n",
    "        writer.writerow([epoch, t_loss, v_loss, mse,psnr,msssim,nrmse])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
